{"cells":[{"cell_type":"markdown","metadata":{"id":"gg2e0GYiqPjk"},"source":["# Homework 3 - Text generation with LSTM and Transformer networks\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6_hn9dHcPKt4"},"source":["## Installs the unidecode library and downloads the Shakespeare dataset."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21195,"status":"ok","timestamp":1714316214793,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"},"user_tz":-120},"id":"3qpdcKW580xG","outputId":"91be2c5e-0da3-4e9a-c7a3-387e5a06ac5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/235.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.8\n","--2024-04-28 14:56:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n","\n","2024-04-28 14:56:55 (54.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}],"source":["!pip install unidecode\n","!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"markdown","metadata":{"id":"1UG5SIDaHKXC"},"source":["## LSTM implementation\n","\n","For this task you will implement the LSTM neural network architecture and train it on the task of character-level text generation. Implement a single layer LSTM and optionally extend your implementation to multiple layers to generate better results.\n","\n","Links:\n","\n","- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html -- Lists the equations for each component of the LSTM cell.\n","- http://colah.github.io/posts/2015-08-Understanding-LSTMs/ -- Intuitive explanation of LSTM\n","- http://karpathy.github.io/2015/05/21/rnn-effectiveness/ -- Explanation and uses of RNNs.\n","\n","\n","Implement the initialization and the forward pass of a LSTMCell and use it as part of the LSTMSimple network class.\n","\n","The input of the LSTM network will be a sequence of characters, whereas the input of the LSTMCell will be a single input character (x), the output of the previous iteration (C) and the hidden state of the previous iteration (h). Iteratively process the entire input character sequence and calculate the loss based on the prediction at each time step.\n","\n","### Do NOT use the torch.nn.LSTM class.\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import Dataset"],"metadata":{"id":"24_rRseNMEff","executionInfo":{"status":"ok","timestamp":1714316218226,"user_tz":-120,"elapsed":3438,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":321,"status":"ok","timestamp":1714316023681,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"},"user_tz":-120},"id":"Ma-reelTqEJ7"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import Dataset\n","\n","class LSTMCell(nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","\n","        super(LSTMCell, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","\n","        ## TODO: Initialize the necessary components\n","        self.forget_gate = nn.Linear(input_dim + hidden_dim, hidden_dim)\n","        self.input_gate = nn.Linear(input_dim + hidden_dim, hidden_dim)\n","        self.cell_gate = nn.Linear(input_dim + hidden_dim, hidden_dim)\n","        self.output_gate = nn.Linear(input_dim + hidden_dim, hidden_dim)\n","\n","    def forward(self, x, C, h):\n","        # x - batch of encoded characters\n","        # C - Cell state of the previous iteration\n","        # h - Hidden state of the previous iteration\n","\n","        # Returns: cell state C_out and the hidden state h_out\n","\n","        #TODO: implement the forward pass of the LSTM cell\n","        #print(x.shape, h.shape)\n","        combined = torch.cat((x, h), 1)\n","\n","        f = torch.sigmoid(self.forget_gate(combined))\n","        i = torch.sigmoid(self.input_gate(combined))\n","        C_tilde = torch.tanh(self.cell_gate(combined))\n","        C = f * C + i * C_tilde\n","        o = torch.sigmoid(self.output_gate(combined))\n","        h = o * torch.tanh(C)\n","\n","        return C, h\n","\n","class LSTMSimple(nn.Module):\n","    def __init__(self, seq_length, input_dim, hidden_dim, output_dim,\n","                 batch_size):\n","        super(LSTMSimple, self).__init__()\n","\n","        self.seq_length = seq_length\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","        self.batch_size = batch_size\n","        self.num_layers = 2\n","\n","        ## TODO: Initialize the LSTM Cell and other potential necessary components\n","        # You can use a nn.Linear layer to project the output of the LSTMCell to\n","        # self.output_dim.\n","        self.lstm_cell = LSTMCell(input_dim, hidden_dim, output_dim)\n","        self.proj = nn.Linear(hidden_dim, output_dim)\n","\n","\n","    def forward(self, x):\n","        # x - One hot encoded batch - Shape: (batch, seq_len, onehot_char)\n","\n","        # Returns the predicted next character for each character in the\n","        # sequence (outputs), also returns the cell state and hidden state of the\n","        # LSTMCell call on the last character. -- outputs, (c,t)\n","\n","        #TODO: Implement the forward pass over the sequenece of characters\n","        batch = x.shape[0]\n","        seq_len = x.shape[1]\n","        C = torch.zeros(batch, self.hidden_dim).cuda()\n","        h = torch.zeros(batch, self.hidden_dim).cuda()\n","        outputs = []\n","\n","        for i in range(seq_len):\n","            char_input = x[:, i, :]\n","            C, h = self.lstm_cell(char_input, C, h)\n","            out = self.proj(h)\n","            outputs.append(out)\n","\n","        outputs = torch.stack(outputs, dim=1)\n","        return outputs, (C, h)\n","\n","class LSTMMultilayer(nn.Module):\n","    def __init__(self, seq_length, input_dim, hidden_dim, output_dim,\n","                 batch_size, layers=1):\n","        super(LSTMMultilayer, self).__init__()\n","\n","        self.seq_length = seq_length\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","        self.batch_size = batch_size\n","        self.num_layers = layers\n","\n","        ## TODO: Initialize the LSTM Cell and other potential necessary components\n","        # You can use a nn.Linear layer to project the output of the LSTMCell to\n","        self.lstm_cells = nn.ModuleList([LSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim, output_dim) for i in range(self.num_layers)])\n","        self.proj = nn.Linear(hidden_dim, output_dim)\n","\n","\n","    def forward(self, x, C=None, h=None):\n","        # x - One hot encoded batch - Shape: (batch, seq_len, onehot_char)\n","\n","        # Returns the predicted next character for each character in the\n","        # sequence (outputs), also returns the cell state and hidden state of the\n","        # LSTMCell call on the last character. -- outputs, (c,t)\n","\n","        #TODO: Implement the forward pass over the sequenece of characters\n","\n","        batch = x.shape[0]\n","        seq_len = x.shape[1]\n","\n","        if C is  None:\n","            C = [torch.zeros(batch, self.hidden_dim).cuda() for _ in range(self.num_layers)]\n","        if h is None:\n","            h = [torch.zeros(batch, self.hidden_dim).cuda() for _ in range(self.num_layers)]\n","\n","        outputs = []\n","\n","        for i in range(seq_len):\n","            char_input = x[:, i, :] # delete  everything in the following for loop for single layer¸\n","            for layer in range(self.num_layers):\n","                #print(char_input.shape, C[layer].shape, h[layer].shape)\n","                if layer == 0:\n","                    C[layer], h[layer] = self.lstm_cells[layer](char_input, C[layer], h[layer])\n","                else:\n","                    C[layer], h[layer] = self.lstm_cells[layer](h[layer-1], C[layer], h[layer])\n","\n","            out = self.proj(h[-1])\n","            outputs.append(out)\n","\n","        outputs = torch.stack(outputs, dim=1)\n","        return outputs, (C, h)\n"]},{"cell_type":"markdown","metadata":{"id":"3A1_UtJrKnU-"},"source":["### LSTM Sampling Code\n","\n","To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n","\n","In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n","\n","In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1714316024473,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"},"user_tz":-120},"id":"X23_lg53Kqj2"},"outputs":[],"source":["def greedy_sampling_lstm(lstm, x, num_chars):\n","    # x -- b x onehot_char\n","    outputs = torch.zeros((1, num_chars, x.shape[2]))\n","    t_outputs, (cell_state, hidden) = lstm(x.float())\n","    for c in range(num_chars):\n","        output_tmp = torch.softmax(lstm.proj(hidden),dim=1)\n","        top_ind = torch.argmax(output_tmp,dim=1)[0]\n","        tmp = torch.zeros_like(x[:,0,:]).cuda()\n","        tmp[:,top_ind] = 1\n","        outputs[:,c] = tmp\n","\n","        cell_state, hidden = lstm.lstm_cell(tmp,cell_state,hidden)\n","    return outputs\n","\n","def topk_sampling_lstm(lstm, x, num_chars):\n","    # x -- b x onehot_char\n","    outputs = torch.zeros((1, num_chars, x.shape[2]))\n","\n","    # Initialize cell states and hidden states for each layer\n","    num_layers = len(lstm.lstm_cells)\n","    cell_states = [torch.zeros(x.shape[0], lstm.hidden_dim).cuda() for _ in range(num_layers)]\n","    hidden_states = [torch.zeros(x.shape[0], lstm.hidden_dim).cuda() for _ in range(num_layers)]\n","\n","    for c in range(num_chars):\n","        for layer in range(num_layers):\n","            if c == 0:\n","                char_input = x[:, 0, :]  # For the first step, use the first character of the sequence\n","            else:\n","                char_input = x[:, c, :]\n","            # Forward pass through LSTM cell for each layer\n","            print(char_input.shape, hidden_states[layer].shape)\n","            if layer == 0:\n","                cell_states[layer], hidden_states[layer] = lstm.lstm_cells[layer](char_input, cell_states[layer], hidden_states[layer])\n","            else:\n","                cell_states[layer], hidden_states[layer] = lstm.lstm_cells[layer](hidden_states[layer - 1], cell_states[layer], hidden_states[layer])\n","\n","        # Perform top-k sampling using the hidden state of the last layer\n","        output_vals, output_ind = torch.topk(lstm.proj(hidden_states[-1]), 5, dim=1)\n","        output_tmp = torch.softmax(output_vals, dim=1)\n","        top_ind = torch.multinomial(output_tmp[0], 1)[0]\n","        tmp = torch.zeros_like(x[:, 0, :]).cuda()\n","        tmp[:, output_ind[0, top_ind]] = 1\n","        outputs[:, c] = tmp\n","\n","        # Update cell states and hidden states for each layer\n","        for layer in range(num_layers):\n","            cell_states[layer], hidden_states[layer] = lstm.lstm_cells[layer](tmp, cell_states[layer], hidden_states[layer])\n","\n","    return outputs\n"]},{"cell_type":"markdown","metadata":{"id":"bJdxAOVsKzBX"},"source":["### LSTM Dataset Code"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1714316218226,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"},"user_tz":-120},"id":"G5U4jzUDK1dG"},"outputs":[],"source":["import unidecode\n","import string\n","import random\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset\n","\n","\n","class LSTMDataset(Dataset):\n","    def __init__(self, chunk_len=200, padded_chunks=False):\n","        # Character based dataset\n","        dataset_path = \"./input.txt\"\n","        # The tokens in the vocabulary (all_characters)\n","        # are just the printable characters of the string class\n","        self.all_characters = string.printable\n","        self.n_characters = len(self.all_characters)\n","        # Maps characters to indices\n","        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n","        self.file, self.file_len = self.read_file(dataset_path)\n","        # Sequence length of the input\n","        self.chunk_len = chunk_len\n","\n","    def read_file(self,filename):\n","        file = unidecode.unidecode(open(filename).read())\n","        return file, len(file)\n","\n","    def char_tensor(self,in_str):\n","        # in_str - input sequence - String\n","        # Return one-hot encoded characters of in_str\n","        tensor = torch.zeros(len(in_str),self.n_characters).long()\n","        char_ind = [self.char_dict[c] for c in in_str]\n","        tensor[torch.arange(tensor.shape[0]),char_ind] = 1\n","        return tensor\n","\n","    def __getitem__(self, idx):\n","        inp, target = self.get_random_text()\n","        return {\"input\":inp, \"target\":target}\n","\n","    def __len__(self):\n","        return 10000\n","\n","    def get_random_text(self):\n","        # Pick a random string of length self.chunk_len from the dataset\n","        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n","        end_index = start_index + self.chunk_len + 1\n","        chunk = self.file[start_index:end_index]\n","        # One-hot encode the chosen string\n","        inp = self.char_tensor(chunk[:-1])\n","        # The target string is the same as the\n","        # input string but shifted by 1 character\n","        target = self.char_tensor(chunk[1:])\n","        inp = Variable(inp).cuda()\n","        target = Variable(target).cuda()\n","        return inp, target\n"]},{"cell_type":"markdown","metadata":{"id":"5S7JDlDZRqrg"},"source":["### LSTM Training loop\n","\n","With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n","especially with the sequence length (chunk_len) used during training."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":842052,"status":"ok","timestamp":1714302675630,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"},"user_tz":-120},"id":"IDVof1Qe1_vG","outputId":"ef43c996-a7da-4da6-cdd5-31d041cb8351"},"outputs":[{"output_type":"stream","name":"stdout","text":["parameters: chunk_len=256, input_dim=100, hidden_dim=256, output_dim=100, batch_size=256\n"]},{"output_type":"stream","name":"stderr","text":["Training - Epoch: 0/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 356.62chunks/s, loss=3.32, lr=0.005, run:=LSTM]\n","Training - Epoch: 1/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 356.65chunks/s, loss=3.28, lr=0.005, run:=LSTM]\n","Training - Epoch: 2/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 355.44chunks/s, loss=2.81, lr=0.005, run:=LSTM]\n","Training - Epoch: 3/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 363.99chunks/s, loss=2.51, lr=0.005, run:=LSTM]\n","Training - Epoch: 4/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 349.81chunks/s, loss=2.34, lr=0.005, run:=LSTM]\n","Training - Epoch: 5/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 363.10chunks/s, loss=2.19, lr=0.005, run:=LSTM]\n","Training - Epoch: 6/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 359.78chunks/s, loss=2.03, lr=0.005, run:=LSTM]\n","Training - Epoch: 7/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 349.28chunks/s, loss=1.92, lr=0.005, run:=LSTM]\n","Training - Epoch: 8/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 360.80chunks/s, loss=1.83, lr=0.005, run:=LSTM]\n","Training - Epoch: 9/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 360.63chunks/s, loss=1.74, lr=0.005, run:=LSTM]\n","Training - Epoch: 10/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 350.25chunks/s, loss=1.68, lr=0.005, run:=LSTM]\n","Training - Epoch: 11/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 362.15chunks/s, loss=1.62, lr=0.005, run:=LSTM]\n","Training - Epoch: 12/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 358.03chunks/s, loss=1.59, lr=0.005, run:=LSTM]\n","Training - Epoch: 13/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 348.63chunks/s, loss=1.55, lr=0.005, run:=LSTM]\n","Training - Epoch: 14/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 361.03chunks/s, loss=1.51, lr=0.005, run:=LSTM]\n","Training - Epoch: 15/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 358.25chunks/s, loss=1.46, lr=0.005, run:=LSTM]\n","Training - Epoch: 16/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 345.46chunks/s, loss=1.47, lr=0.005, run:=LSTM]\n","Training - Epoch: 17/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 353.67chunks/s, loss=1.45, lr=0.005, run:=LSTM]\n","Training - Epoch: 18/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 352.90chunks/s, loss=1.41, lr=0.005, run:=LSTM]\n","Training - Epoch: 19/30: 100%|█████████▉| 9984/10000 [00:29<00:00, 342.04chunks/s, loss=1.4, lr=0.005, run:=LSTM]\n","Training - Epoch: 20/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 358.59chunks/s, loss=1.39, lr=0.005, run:=LSTM]\n","Training - Epoch: 21/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 354.58chunks/s, loss=1.37, lr=0.005, run:=LSTM]\n","Training - Epoch: 22/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 345.74chunks/s, loss=1.37, lr=0.005, run:=LSTM]\n","Training - Epoch: 23/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 355.99chunks/s, loss=1.36, lr=0.005, run:=LSTM]\n","Training - Epoch: 24/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 359.74chunks/s, loss=1.37, lr=0.005, run:=LSTM]\n","Training - Epoch: 25/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 350.98chunks/s, loss=1.33, lr=0.005, run:=LSTM]\n","Training - Epoch: 26/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 363.97chunks/s, loss=1.34, lr=0.005, run:=LSTM]\n","Training - Epoch: 27/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 358.71chunks/s, loss=1.32, lr=0.005, run:=LSTM]\n","Training - Epoch: 28/30: 100%|█████████▉| 9984/10000 [00:28<00:00, 356.42chunks/s, loss=1.31, lr=0.005, run:=LSTM]\n","Training - Epoch: 29/30: 100%|█████████▉| 9984/10000 [00:27<00:00, 363.37chunks/s, loss=1.29, lr=0.005, run:=LSTM]\n"]}],"source":["from tqdm import tqdm\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import StepLR\n","\n","batch_size = 256\n","chunk_len = 256\n","model_name = \"LSTM\"\n","train_dataset = LSTMDataset(chunk_len=chunk_len)\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, drop_last=True)\n","\n","#Sample parameters, use whatever you see fit.\n","input_dim = train_dataset.n_characters\n","hidden_dim = 256\n","output_dim = train_dataset.n_characters\n","learning_rate = 0.005\n","#model = LSTMSimple(chunk_len,input_dim, hidden_dim, output_dim,batch_size)\n","model = LSTMMultilayer(chunk_len,input_dim, hidden_dim, output_dim, batch_size, layers=2)\n","model.train()\n","model.cuda()\n","print(f\"parameters: chunk_len={chunk_len}, input_dim={input_dim}, hidden_dim={hidden_dim}, output_dim={output_dim}, batch_size={batch_size}\")\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","#scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n","epochs=30\n","\n","for epoch in range(epochs):\n","    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n","        for i, data in enumerate(trainloader, 0):\n","            inputs = data['input'].float()\n","            labels = data['target'].float()\n","            # b x chunk_len x len(dataset.all_characters)\n","            target = torch.argmax(labels, dim=2)\n","            optimizer.zero_grad()\n","            outputs, _ = model(inputs)\n","            #print()\n","            #print(\"inputs shape\", inputs.shape)\n","            #print(\"labels shape\", labels.shape)\n","            #print(\"outputs shape\", outputs.shape)\n","            #print(\"target shape\", target.shape)\n","            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1), target.view(labels.shape[0]*labels.shape[1]))\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n","            optimizer.step()\n","            prog_bar.set_postfix(**{'run:': model_name,'lr': learning_rate,\n","                                    'loss': loss.item()})\n","            prog_bar.update(batch_size)\n","        # Intermediate output\n","        \"\"\"\n","        sample_text = \"O Romeo, wherefore art thou\"\n","        inp = train_dataset.char_tensor(sample_text)\n","        sample_input = Variable(inp).cuda().unsqueeze(0).float()\n","        print(\"\\n\", sample_input.shape)\n","        out_test = topk_sampling_lstm(model, sample_input, 300)[0]\n","        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n","        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n","        print(\"Top-K sampling -----------------\")\n","        print(out_chars)\n","\n","        out_test = greedy_sampling_lstm(model,sample_input, 300)[0]\n","        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n","        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n","        print(\"Greedy sampling ---------------\")\n","        print(out_chars)\"\"\"\n"]},{"cell_type":"code","source":["import torch\n","from torch.autograd import Variable\n","\n","def greedy_sampling_lstm_multilayer(lstm, x, num_chars):\n","    # x -- b x onehot_char\n","    outputs = torch.zeros((1, num_chars, x.shape[2]))\n","    t_outputs, (cell_state, hidden) = lstm(x.float())\n","\n","    for c in range(num_chars):\n","        #print(hidden.shape)\n","        output_tmp = torch.softmax(lstm.proj(hidden[-1]),dim=1)\n","        top_ind = torch.argmax(output_tmp,dim=1)[0]\n","        tmp = torch.zeros_like(x[:,0,:]).cuda()\n","        tmp[:,top_ind] = 1\n","        outputs[:,c] = tmp\n","        #print(\"tmp:\", tmp.shape)\n","        t_outputs, (cell_state, hidden) = lstm(tmp.view(1, 1, 100), cell_state, hidden)\n","\n","    return outputs\n","\n","def topk_sampling_lstm_multilayer(lstm, x, num_chars):\n","    # x -- b x onehot_char\n","    outputs = torch.zeros((1,num_chars,x.shape[2]))\n","    t_outputs, (cell_state, hidden) = lstm(x.float())\n","\n","    for c in range(num_chars):\n","        output_vals, output_ind = torch.topk(lstm.proj(hidden[-1]), 5, dim=1)\n","        output_tmp = torch.softmax(output_vals,dim=1)\n","        top_ind = torch.multinomial(output_tmp[0], 1)[0]\n","        tmp = torch.zeros_like(x[:,0,:]).cuda()\n","        tmp[:,output_ind[0,top_ind]] = 1\n","        outputs[:,c] = tmp\n","\n","        t_outputs, (cell_state, hidden) = lstm(tmp.view(1, 1, 100), cell_state, hidden)\n","\n","    return outputs\n","\n","# Sample usage\n","sample_text = \"O Romeo, wherefore art thou\"\n","inp = train_dataset.char_tensor(sample_text)\n","sample_input = Variable(inp).cuda().unsqueeze(0).float()\n","print(\"\\n\", sample_input.shape)\n","\n","out_test = topk_sampling_lstm_multilayer(model, sample_input, 500)[0]\n","out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n","out_chars = sample_text + \"\".join([train_dataset.all_characters[i] for i in out_char_index])\n","print(\"Top-K sampling -----------------\")\n","print(out_chars)\n","print(\"\\n\\n ------------------------------------------------ \\n\\n\")\n","out_test = greedy_sampling_lstm_multilayer(model, sample_input, 500)[0]\n","out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n","out_chars = sample_text + \"\".join([train_dataset.all_characters[i] for i in out_char_index])\n","print(\"Greedy sampling ---------------\")\n","print(out_chars)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4g5ZbXnlaUUp","executionInfo":{"status":"ok","timestamp":1714302803398,"user_tz":-120,"elapsed":2419,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}},"outputId":"a244b916-108e-43e3-d7c4-7ebf526e16c9"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," torch.Size([1, 27, 100])\n","Top-K sampling -----------------\n","O Romeo, wherefore art thou this absolute,\n","Stand at your grations and made you are to\n","my father.\n","\n","PETRUCHIO:\n","Not with that thou a man on him; I cannot,\n","So shower he willingly friends o' the harrius\n","Which hath trather shall be assay.\n","\n","CORIOLANUS:\n","Ay, at all their horse.' 'Tis minis a sight:\n","But that said o' the struck that here, so fair she\n","This man's liege, but I thou att my heart.\n","\n","BUCKINGHAM:\n","I do thou art all that\n","I'll there, my fare head a free hopes of you.\n","\n","Provost\n","Must have been to to spake thee at his highness;\n","An\n","\n","\n"," ------------------------------------------------ \n","\n","\n","Greedy sampling ---------------\n","O Romeo, wherefore art thou shalt see the seas,\n","And therefore he shall be so many things and so,\n","That I will be so much a thousand for thee,\n","And therefore he shall be so many things and so,\n","That I will be so much a thousand for thee,\n","And therefore he shall be so many things and so,\n","That I will be so much a thousand for thee,\n","And therefore he shall be so many things and so,\n","That I will be so much a thousand for thee,\n","And therefore he shall be so many things and so,\n","That I will be so much a thousand for thee,\n","And therefore \n"]}]},{"cell_type":"markdown","metadata":{"id":"UwQEM1JHzDo-"},"source":["# Task 2: Character generation transformer network implementation\n","Our simple transformer-like network will take as input a sequence of characters and predict the next character in the sequence. To ensure an efficient training procedure, masked attention modules will be used as in the [GPT model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).\n","\n","For this task you must implement the Scaled dot product attention module and the Masked multi-head attention module. Both of these modules are described in the [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper (See Figure 2 in the paper as well as Sections 3.2.1, 3.2.2 and 3.2.3). They are the core operations of transformers. As we will use our model for text generation also add the masking operation shown as (mask opt.) in Figure 2, implemented as AttentionMasking in the code.\n","\n","**Implement the modules in the ScaledDotProductAttention class and the MultiHeadAttention class.**\n","\n","Read the GPT paper and the Attention is all you need paper for a better understanding of the components. For a more high level overview, this [post](https://jalammar.github.io/illustrated-gpt2/) may also be helpful.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"lNhqnTm8zGRe","executionInfo":{"status":"ok","timestamp":1714316218227,"user_tz":-120,"elapsed":9,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import Dataset\n","import math\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=1000):\n","        super().__init__()\n","        # Positional encoding adds the positional information to the\n","        # embedding. Without it the model would not be able to differentiate\n","        # between different characters orders such as between \"dog\" and \"god\".\n","        position = torch.arange(max_len).unsqueeze(1).float()\n","        div_term = 10000.0**(torch.arange(0,d_model,2).float()/d_model)\n","        print(div_term.shape)\n","        pe = torch.zeros(max_len, d_model)\n","        pe[:, 0::2] = torch.sin(position / div_term)\n","        pe[:, 1::2] = torch.cos(position / div_term)\n","        pe = pe.unsqueeze(0)\n","        self.pe = pe.cuda()\n","        self.pe.requires_grad = False\n","\n","    def forward(self, x):\n","        p = self.pe[:, :x.size(1)]\n","        return p\n","\n","class AttentionMasking(nn.Module):\n","    def __init__(self, max_len):\n","        super(AttentionMasking, self).__init__()\n","        self.register_buffer(\"mask\", torch.tril(torch.ones(max_len, max_len))\n","                                     .view(1, 1, max_len, max_len))\n","    def forward(self,x):\n","        length = x.shape[-1]\n","        out = x.masked_fill(self.mask[:,:,:length,:length] == 0, float('-inf'))\n","        return out\n","\n","\n","class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, max_len):\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.softmax = nn.Softmax(dim=-1)\n","        # Multiply with an upper triangular\n","        # matrix of dimensions (length x length) after the scale operation\n","        # in Figure 2 of the paper.\n","        self.mask_opt = AttentionMasking(max_len)\n","\n","    def forward(self, q, k, v):\n","        # length = number of input tokens\n","        batch_size,num_heads,length,num_neuron = k.size()\n","        # TODO: Implement the scaled dot product attention as described in\n","        # the Attention is all you need paper in Equation 1\n","\n","        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))\n","        scores = self.mask_opt(scores)\n","        attention_weights = self.softmax(scores)\n","        output = torch.matmul(attention_weights, v)\n","        return output\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, dim_model, num_neuron, n_head, max_len):\n","        super(MultiHeadAttention, self).__init__()\n","        self.dim_model = dim_model\n","        self.n_head = n_head\n","        self.num_neuron = num_neuron\n","\n","        # TODO: Initialize the ScaledDotProductAttention and other\n","        # necessary components.\n","        self.sdp_attention = ScaledDotProductAttention(max_len)\n","        self.query_projection = nn.Linear(dim_model, num_neuron * n_head)\n","        self.key_projection = nn.Linear(dim_model, num_neuron * n_head)\n","        self.value_projection = nn.Linear(dim_model, num_neuron * n_head)\n","        self.concat_projection = nn.Linear(num_neuron * n_head, dim_model)\n","\n","    def split(self,tensor):\n","        batch_size, length, total_dim = tensor.size()\n","        # Reshape the tensor to enable the use in\n","        # the ScaledDotProductAttention module\n","        split_tensor = tensor.view(batch_size, length, self.n_head, self.num_neuron).transpose(1,2)\n","        return split_tensor\n","\n","    def concat(self,tensor):\n","        batch_size, num_heads, length, num_neuron = tensor.size()\n","        # Reshape the tensor to its original size before the split operation.\n","        concat_tensor = tensor.transpose(1,2).contiguous().view(batch_size, length, self.n_head*self.num_neuron)\n","        return concat_tensor\n","\n","    def forward(self, q, k, v):\n","        # TODO: Implement the Masked Multi-head attention module as described in the\n","        # Attention is all you need paper in Figure 1 and Section 3.2.2.\n","        batch_size = q.shape[0]\n","        #print(\"q before: \", q.shape)\n","        q = self.query_projection(q)\n","        k = self.key_projection(k)\n","        v = self.value_projection(v)\n","        #print(\"q after proj: \", q.shape)\n","\n","        q = self.split(q)\n","        k = self.split(k)\n","        v = self.split(v)\n","        #print(\"q after split: \", q.shape)\n","\n","        attention_output = self.sdp_attention(q, k, v)\n","        #print(\"1 \\t\", attention_output.shape)\n","        attention_output = self.concat(attention_output)\n","        #print(\"2 \\t\",attention_output.shape)\n","        output = self.concat_projection(attention_output)\n","        return output\n","\n","class PositionFeedForwardNet(nn.Module):\n","    def __init__(self, dim_model):\n","        super(PositionFeedForwardNet, self).__init__()\n","        self.ff_net1 = nn.Linear(dim_model, dim_model*4)\n","        self.ff_net2 = nn.Linear(dim_model*4, dim_model)\n","\n","    def forward(self,x):\n","        ff_out = self.ff_net1(x)\n","        ff_out = torch.nn.functional.relu(ff_out)\n","        ff_out = self.ff_net2(ff_out)\n","        return ff_out\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, dim_model, num_neuron, n_head, max_len):\n","        super(TransformerBlock, self).__init__()\n","        self.mha = MultiHeadAttention(dim_model, num_neuron, n_head, max_len)\n","        self.l_norm = torch.nn.LayerNorm(dim_model)\n","        self.l_norm2 = torch.nn.LayerNorm(dim_model)\n","        self.ff_net = PositionFeedForwardNet(dim_model)\n","        # b, len_seq, n_head, num_neuron\n","\n","    def forward(self, x):\n","      # A Transformer block as described in the\n","      # Attention is all you need paper. In Figure 1 the transformer\n","      # block is marked with a gray rectangle right of the text \"Nx\"\n","      _x = x\n","      mha1 = self.mha(x,x,x)\n","      lnorm = self.l_norm(_x + mha1)\n","      _x = lnorm\n","      ff_out = self.ff_net(lnorm)\n","      out = self.l_norm2(ff_out + _x)\n","\n","      return out\n","\n","class TransformerSimple(nn.Module):\n","    def __init__(self, seq_length, input_dim, output_dim,\n","                 batch_size):\n","        super(TransformerSimple, self).__init__()\n","        num_neuron = 64\n","        n_head = 8\n","        dim_model=256\n","        max_len = 512\n","        self.start_embedding = nn.Embedding(input_dim, dim_model)\n","\n","        self.pos_embedding = PositionalEncoding(dim_model)\n","\n","        # b x l x c*n_head\n","        self.t_block1 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n","        self.t_block2 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n","        self.t_block3 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n","        self.t_block4 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n","        self.t_block5 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n","\n","        #self.t_block6 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n","        #self.t_block7 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n","        #self.t_block8 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n","\n","        #self.out_layer_1 = nn.Linear(dim_model, dim_model)\n","        self.output_layer = nn.Linear(dim_model,output_dim)\n","\n","    def forward(self,x):\n","      # x - Tensor - (b, seq_len)\n","      # Embeds the input tensor from tokens to features\n","      s_emb = self.start_embedding(x)\n","      # Adds positional embeddings\n","      p_emb = self.pos_embedding(s_emb)\n","      b_out = p_emb + s_emb\n","      # Transformer blocks - You can experiment with varying depth\n","      # For example GPT uses 12 blocks but this might be a bit memory intensive\n","      b_out = self.t_block1(b_out)\n","      b_out = self.t_block2(b_out)\n","      b_out = self.t_block3(b_out)\n","      b_out = self.t_block4(b_out)\n","      b_out = self.t_block5(b_out)\n","\n","      #b_out = self.t_block6(b_out)\n","      #b_out = self.t_block7(b_out)\n","      #b_out = self.t_block8(b_out)\n","\n","      # Output mapping to a classification of output tokens\n","      # For each token the network tries to predict the next token\n","      # based only on the previous tokens.\n","      # Output shape: (b x seq_len x vocabulary_size)\n","      out = self.output_layer(b_out)\n","\n","      return out\n"]},{"cell_type":"markdown","metadata":{"id":"HPjl6ttzPHsq"},"source":["## Dataset class\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"T04eYePr8Gn3","executionInfo":{"status":"ok","timestamp":1714316218227,"user_tz":-120,"elapsed":8,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}}},"outputs":[],"source":["import unidecode\n","import string\n","import random\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset\n","\n","class TextDataset(Dataset):\n","    def __init__(self, chunk_len=200, padded_chunks=False):\n","        # Character based dataset\n","        dataset_path = \"./input.txt\"\n","        # The tokens in the vocabulary (all_characters)\n","        # are just the printable characters of the string class\n","        self.all_characters = string.printable\n","        self.n_characters = len(self.all_characters)\n","        # Maps characters to indices\n","        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n","        self.file, self.file_len = self.read_file(dataset_path)\n","        # Sequence length of the input\n","        self.chunk_len = chunk_len\n","        self.encoded_file = [self.char_dict[x] for x in self.file]\n","\n","    def read_file(self,filename):\n","        file = unidecode.unidecode(open(filename).read())\n","        return file, len(file)\n","\n","    def encode_text(self,in_str):\n","        # in_str - input sequence - String\n","        # Returns - in_str mapped to tokens in char_dict\n","        tensor = torch.LongTensor([self.char_dict[x] for x in in_str])\n","        return tensor\n","\n","    def __getitem__(self, idx):\n","        inp, target = self.get_random_text()\n","        return {\"input\":inp, \"target\":target}\n","\n","    def __len__(self):\n","        return 10000\n","\n","    def get_random_text(self):\n","        # Pick a random string of length self.chunk_len from the dataset\n","        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n","        end_index = start_index + self.chunk_len + 1\n","        chunk = self.encoded_file[start_index:end_index]\n","        # input_tokens - random sequence of tokens from the dataset\n","        input_tokens = torch.LongTensor(chunk[:-1])\n","        # target - input token sequence shifted by 1\n","        # the idea is to predict next token for each token in the input sequence\n","        # therefore if the input is [1,2,3,4] the target is [2,3,4,5]\n","        target = torch.LongTensor(chunk[1:])\n","        input_tokens = input_tokens.cuda()\n","        target = target.cuda()\n","        return input_tokens, target\n"]},{"cell_type":"markdown","metadata":{"id":"JhvayVAjsCMf"},"source":["## Character sampling\n","\n","To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n","\n","In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n","\n","In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"3IVliOUqqEd5","executionInfo":{"status":"ok","timestamp":1714316218227,"user_tz":-120,"elapsed":8,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}}},"outputs":[],"source":["def topk_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n","    # x -- b x onehot_char\n","    # x = b x l\n","    outputs = torch.zeros((1,num_chars))\n","    inp = x\n","\n","    for t in range(num_chars):\n","        # b x onehot_char\n","        output = model(inp.long())[0,-1:]\n","        #output = torch.softmax(output, dim=1)\n","        # b x 3\n","        output_vals, output_ind = torch.topk(output, 5, dim=1)\n","        # 3 -> int\n","        output_vals = torch.softmax(output_vals, dim=1)\n","        top_ind = torch.multinomial(output_vals[0], 1)[0]\n","        # int\n","        out_char_index = output_ind[0,top_ind]\n","        # int -> 1\n","        out_char_index = torch.ones(1).cuda() * out_char_index\n","\n","        outputs[:,t] = out_char_index.item()\n","        if inp.shape[1] > chunk_len:\n","          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n","        else:\n","          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n","\n","    return outputs\n","\n","\n","def greedy_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n","    # x -- shape (batch, tokens in x)\n","    outputs = torch.zeros((1,num_chars))\n","    inp = x\n","\n","    for t in range(num_chars):\n","        # b x l x onehot_char\n","        output = model(inp.long())[0,-1:]\n","        output = torch.softmax(output, dim=1)\n","        out_char_index = torch.argmax(output, dim=1)\n","        outputs[:,t] = out_char_index.item()\n","        if inp.shape[1] > chunk_len:\n","          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n","        else:\n","          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n","\n","    return outputs\n"]},{"cell_type":"markdown","metadata":{"id":"s6X7-Tlc2iqh"},"source":["## Transformer model training\n","\n","With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n","especially with the sequence length (chunk_len) used during training."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"gY8aZz1R2g3M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714319228753,"user_tz":-120,"elapsed":427878,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}},"outputId":"8907085e-f787-4189-a7b9-0cc55537ad4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([128])\n"]},{"output_type":"stream","name":"stderr","text":["Training - Epoch: 0/50: 10240chunks [00:08, 1201.30chunks/s, loss=2.49, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 1/50: 10240chunks [00:08, 1192.50chunks/s, loss=2.36, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 2/50: 10240chunks [00:08, 1172.88chunks/s, loss=2.11, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 3/50: 10240chunks [00:08, 1159.13chunks/s, loss=2.09, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 4/50: 10240chunks [00:08, 1180.10chunks/s, loss=1.89, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 5/50: 10240chunks [00:08, 1188.31chunks/s, loss=1.78, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 6/50: 10240chunks [00:08, 1197.58chunks/s, loss=1.72, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 7/50: 10240chunks [00:08, 1214.25chunks/s, loss=1.57, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 8/50: 10240chunks [00:08, 1210.05chunks/s, loss=1.58, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 9/50: 10240chunks [00:08, 1209.76chunks/s, loss=1.47, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 10/50: 10240chunks [00:08, 1208.78chunks/s, loss=1.5, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 11/50: 10240chunks [00:08, 1203.04chunks/s, loss=1.55, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 12/50: 10240chunks [00:08, 1184.48chunks/s, loss=1.49, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 13/50: 10240chunks [00:08, 1179.98chunks/s, loss=1.55, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 14/50: 10240chunks [00:08, 1205.87chunks/s, loss=1.45, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 15/50: 10240chunks [00:08, 1190.44chunks/s, loss=1.42, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 16/50: 10240chunks [00:08, 1201.53chunks/s, loss=1.41, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 17/50: 10240chunks [00:08, 1217.45chunks/s, loss=1.34, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 18/50: 10240chunks [00:08, 1199.43chunks/s, loss=1.29, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 19/50: 10240chunks [00:08, 1196.91chunks/s, loss=1.39, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 20/50: 10240chunks [00:08, 1199.22chunks/s, loss=1.33, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 21/50: 10240chunks [00:08, 1194.50chunks/s, loss=1.38, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 22/50: 10240chunks [00:08, 1195.51chunks/s, loss=1.44, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 23/50: 10240chunks [00:08, 1194.59chunks/s, loss=1.29, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 24/50: 10240chunks [00:08, 1188.44chunks/s, loss=1.34, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 25/50: 10240chunks [00:08, 1195.06chunks/s, loss=1.32, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 26/50: 10240chunks [00:08, 1193.28chunks/s, loss=1.28, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 27/50: 10240chunks [00:08, 1210.92chunks/s, loss=1.21, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 28/50: 10240chunks [00:08, 1194.07chunks/s, loss=1.18, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 29/50: 10240chunks [00:08, 1195.84chunks/s, loss=1.27, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 30/50: 10240chunks [00:08, 1209.11chunks/s, loss=1.28, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 31/50: 10240chunks [00:08, 1194.79chunks/s, loss=1.23, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 32/50: 10240chunks [00:08, 1195.94chunks/s, loss=1.26, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 33/50: 10240chunks [00:08, 1202.67chunks/s, loss=1.24, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 34/50: 10240chunks [00:08, 1203.69chunks/s, loss=1.18, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 35/50: 10240chunks [00:08, 1196.10chunks/s, loss=1.26, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 36/50: 10240chunks [00:08, 1194.24chunks/s, loss=1.23, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 37/50: 10240chunks [00:08, 1210.36chunks/s, loss=1.19, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 38/50: 10240chunks [00:08, 1199.42chunks/s, loss=1.12, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 39/50: 10240chunks [00:08, 1193.16chunks/s, loss=1.19, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 40/50: 10240chunks [00:08, 1210.07chunks/s, loss=1.24, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 41/50: 10240chunks [00:08, 1200.39chunks/s, loss=1.16, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 42/50: 10240chunks [00:08, 1193.32chunks/s, loss=1.22, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 43/50: 10240chunks [00:08, 1206.44chunks/s, loss=1.1, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 44/50: 10240chunks [00:08, 1199.48chunks/s, loss=1.11, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 45/50: 10240chunks [00:08, 1196.68chunks/s, loss=1.22, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 46/50: 10240chunks [00:08, 1205.14chunks/s, loss=1.16, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 47/50: 10240chunks [00:08, 1204.34chunks/s, loss=1.2, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 48/50: 10240chunks [00:08, 1197.49chunks/s, loss=1.11, lr=0.0006, run:=Transformer]                      \n","Training - Epoch: 49/50: 10240chunks [00:08, 1194.94chunks/s, loss=1.04, lr=0.0006, run:=Transformer]                      \n"]}],"source":["from tqdm import tqdm\n","import torch.optim as optim\n","\n","#Sample parameters, use whatever you see fit.\n","batch_size = 256\n","chunk_len = 64\n","train_dataset = TextDataset(chunk_len=chunk_len)\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n","\n","input_dim = train_dataset.n_characters\n","output_dim = train_dataset.n_characters\n","learning_rate = 0.0006\n","\n","model = TransformerSimple(chunk_len, input_dim, output_dim,batch_size)\n","model.train()\n","model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","epochs=50\n","\n","for epoch in range(epochs):\n","    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n","        for i, data in enumerate(trainloader, 0):\n","            # inputs - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n","            inputs = data['input'].long()\n","            # labels - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n","            labels = data['target'].long()\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            target_t = labels\n","            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1),target_t.view(labels.shape[0]*labels.shape[1]))\n","            loss.backward()\n","            optimizer.step()\n","            prog_bar.set_postfix(**{'run:': \"Transformer\", 'lr': learning_rate,\n","                                    'loss': loss.item()\n","                                    })\n","            prog_bar.update(batch_size)\n","\n","        # Intermediate text output\n","        \"\"\"\n","        sample_texts = [\"What authority surfeits on\",\n","                        \"I say unto you, what he hath done famously, he did it to that end:\",\n","                        \"That in submission will return to us: And then, as we have ta'en the sacrament,\"]\n","        output_token = torch.zeros(1,1).cuda()\n","        output_token[0,0] = train_dataset.n_characters-1\n","        print(\"Top-K sampling\")\n","        for sample_text in sample_texts:\n","            sample_encoding = train_dataset.encode_text(sample_text)\n","            sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n","\n","            #out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n","            out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n","            out_char_index = out_test.long().detach().cpu().numpy()\n","            out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n","            print(\"----------------------------------------\")\n","            print(out_chars)\"\"\"\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y-Tyjm1kHz5Z"},"source":["## Text sampling - Transformers\n"]},{"cell_type":"code","source":["sample_text = \"Here's to my love! O true apothecary! Thy drugs are quick.\"\n","sample_encoding = train_dataset.encode_text(sample_text)\n","sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n","output_token = torch.zeros(1,1).cuda()\n","output_token[0,0] = train_dataset.n_characters-1"],"metadata":{"id":"QOf1v7xi1s_I","executionInfo":{"status":"ok","timestamp":1714319262550,"user_tz":-120,"elapsed":747,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{"id":"XjvbjEdjH36Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714319268608,"user_tz":-120,"elapsed":6065,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}},"outputId":"8d045be9-08b5-4cfe-bba4-c541adf54bd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Top-K Sampling:\n","----------------------------------------\n","Here's to my love! O true apothecary! Thy drugs are quick. \n","\n","GLOUCESTER:\n","I do believe this wretch charrged willl death.\n","\n","\n","LUCIO:\n","These greatsorse satisfy to his person, and himself\n","How sailty too beast all posssed; when he stands he\n","gentle haste, with a winged bloow the cause of this place,\n","Which, wert thou come\n","Wate home: though I have sperit\n","In posssible ass stem, intrussed me\n","IntreaN, and alll the world, thou wilt plead,\n","To fear off the starm of hell a\n","\n","Greedy Sampling\n","----------------------------------------\n","Here's to my love! O true apothecary! Thy drugs are quick. \n","\n","KING EDWARD IV:\n","No, Greough: nor go fine hath as steep ass\n","Werst fear the pent to off an oath?\n","O my wife, thou seen me not? Well will are you?\n","\n","\n","Pedast:\n","Your worrrong me nor grant to you come;\n","For such ffair aunt or a place of the dead.\n","I wish me tome means, and I him, and welll\n","Condemned, to me to them and the placefe of the world:\n","When I shalll, be sadd, the sigester is alll.\n","The word of Gor h\n"]}],"source":["print(\"Top-K Sampling:\")\n","out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n","out_char_index = out_test.long().detach().cpu().numpy()\n","out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n","print(\"----------------------------------------\")\n","print(out_chars)\n","print()\n","print(\"Greedy Sampling\")\n","out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n","out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n","out_char_index = out_test.long().detach().cpu().numpy()\n","out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n","print(\"----------------------------------------\")\n","print(out_chars)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"zdztLMsP2AiL","executionInfo":{"status":"aborted","timestamp":1714315706564,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install numba\n","\n","from numba import cuda\n","device = cuda.get_current_device()\n","device.reset()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0ZR6c4rKqOk","executionInfo":{"status":"ok","timestamp":1714316133930,"user_tz":-120,"elapsed":7223,"user":{"displayName":"Nejc L.","userId":"10283319336934705075"}},"outputId":"a7b782dc-4251-4302-c039-536d817cfbe9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.58.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.41.1)\n","Requirement already satisfied: numpy<1.27,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.25.2)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ontn9qKHKqmo"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1zd7qQjjuDljWTZoczIAvfZ1dfz3zZ49X","timestamp":1714065883012},{"file_id":"1LmPkuBaWGBWG9eX50L6iBJWhg5j-gsVZ","timestamp":1712647640248},{"file_id":"1_eTwdHI-t9mZhLEYVdg15KUSVlj1cVos","timestamp":1650840454544},{"file_id":"1x2TNSX1w0w9_CspJ8CkvcIX9xVzhA_AN","timestamp":1650655532871},{"file_id":"1RhajfAv5Ek2iVBjdUKcEg-9znmS3aga8","timestamp":1650579410008},{"file_id":"1NXb7NB2NMR_8ZgirIrP1O95auesuwuqI","timestamp":1650530117488},{"file_id":"1ka_IzRoDL8a6gpt_SQYTgku50sXWU_7Y","timestamp":1649682627533},{"file_id":"1nSEAHa3KkRrQkG1i5Bj8wzGZixoEKQys","timestamp":1619607058631}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}